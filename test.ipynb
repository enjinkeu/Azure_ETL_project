{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from utils import *\n",
    "import sys\n",
    "\n",
    "sys.modules['pkg_resources'] = None\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#                                                #   \n",
    "# this file will store all the utility functions #\n",
    "#                                                #\n",
    "#################################################\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import hashlib\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from time import time, sleep\n",
    "from typing import List\n",
    "\n",
    "import openai\n",
    "import pinecone\n",
    "import pdfplumber\n",
    "import tiktoken\n",
    "import pymongo\n",
    "from tika import parser\n",
    "from unidecode import unidecode\n",
    "from azure.cosmos import exceptions, CosmosClient, PartitionKey\n",
    "from azure.storage.blob import BlobServiceClient, ContainerClient, BlobClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.cosmos import CosmosClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import *\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cli_signIn():\n",
    "    logging.info(\"signing in to azure cli\")    \n",
    "    os.system(\"az account list --output tsv | grep True -q || az login\")\n",
    "\n",
    "######################    SECRETS       ###############################\n",
    "\n",
    "def get_env_vars():\n",
    "    logging.info(\"getting environment variables\")\n",
    "\n",
    "    env_dict = {\n",
    "        \"resource_group_name\": 'chatgptGp',\n",
    "        \"storage_account_name\": 'chatgptv2stn',\n",
    "        \"container_name\": 'chatgpt-ctn',\n",
    "        \"cosmosdb_acc\": 'chatgptdb-acn',\n",
    "        \"database_name\": 'chatgptdb-dbn',\n",
    "        \"collection_name\": 'chatgptdb-cln' ,\n",
    "        \"OPENAI_API_KEY\": get_secret(keyvault_name=\"chatkeys\",secret_name=\"openaiKey\"),\n",
    "        \"connection_string\": os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group 'chatgptGp'\\\n",
    "                              --name chatgptdb-acn | jq .connectionStrings[0].connectionString \").read().strip().replace('\"',''),\n",
    "    }   \n",
    "    for k, v in env_dict.items():\n",
    "        if v is None:\n",
    "            logging.error(f\"{k} environment variable is not set\")\n",
    "            raise Exception(f\"{k} environment variable is not set\")\n",
    "    return env_dict\n",
    "\n",
    "\n",
    "def get_secret(keyvault_name =\"chatkeys\", secret_name = \"openaiKey\"):\n",
    "    \n",
    "    logging.info(f\"getting secret {secret_name} from keyvault {keyvault_name}\")    \n",
    "    credential = DefaultAzureCredential()\n",
    "    secret_client = SecretClient(vault_url=f\"https://{keyvault_name}.vault.azure.net\", credential=credential)\n",
    "    secret = secret_client.get_secret(secret_name)\n",
    "    return secret.value\n",
    "\n",
    "def get_pinecone_keys():\n",
    "    logging.info(\"getting pinecone keys from keyvault\")\n",
    "    \"\"\" get pinecone keys from azure\"\"\"\n",
    "    pinecone_api_key = get_secret(keyvault_name =\"chatkeys\", secret_name = \"pinecone\")\n",
    "    pinecone_env = get_secret(keyvault_name =\"chatkeys\", secret_name = \"pineconeEnv\")\n",
    "    pinecone_index = get_secret(keyvault_name=\"chatKeys\", secret_name=\"pineconeIdx\")\n",
    "\n",
    "    return {\n",
    "        \"pinecone.apiKey\": pinecone_api_key,\n",
    "        \"pinecone.environment\": pinecone_env,\n",
    "        \"pinecone.indexName\": pinecone_index,\n",
    "        \"pinecone.projectName\": \"chatgpt3\"        \n",
    "    }\n",
    "        \n",
    "def get_cosmosdb_keys():\n",
    "    logging.info(\"getting cosmosdb keys from keyvault\")    \n",
    "    \n",
    "    #create environment variables      \n",
    "    resourceGroup = get_env_vars()['resource_group_name']\n",
    "    cosmosdb_name = get_env_vars()['cosmosdb_acc']\n",
    "    cosmosDbEndpoint_url = os.popen(f\"az cosmosdb show --resource-group {resourceGroup}  --name {cosmosdb_name} --query 'writeLocations[].documentEndpoint' -o tsv\").read().strip()\n",
    "    cosmos_account_key =   os.popen(f\"az cosmosdb keys  list --name {cosmosdb_name} --resource-group {resourceGroup} | jq -r '.primaryMasterKey'\").read().strip()    \n",
    "    database_name =        os.popen(f\"az cosmosdb database list --name {cosmosdb_name} --resource-group {resourceGroup} | jq -r '.[0].id'\").read().strip()\n",
    "    collection_name =       os.popen(f\"az cosmosdb collection list --name {cosmosdb_name} --db-name {database_name} --resource-group {resourceGroup} | jq -r '.[0].id'\").read().strip()\n",
    "    masterkey =            os.popen(f\" az cosmosdb list-keys --name {cosmosdb_name} --resource-group {resourceGroup} --query primaryMasterKey\").read().strip()\n",
    "    connection_string =    os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group {resourceGroup}\\\n",
    "                            --name {cosmosdb_name} | jq '.connectionStrings[0].connectionString' \").read().strip().replace('\"','')\n",
    "    \n",
    "    return {\n",
    "            \n",
    "            \"cosmosDbEndpoint_url\" : cosmosDbEndpoint_url,\n",
    "            \"masterkey\" : masterkey,\n",
    "            \"database_name\" : database_name,\n",
    "            \"collection_name\" : collection_name,\n",
    "            \"connection_string\" : connection_string\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def set_spark_liraries():\n",
    "    \n",
    "    logging.info(\"setting spark libraries\")\n",
    "    #packages to load in spark session\n",
    "    group_id = \"io.pinecone\"\n",
    "    artifact_id = \"spark-pinecone_2.13\"\n",
    "    version = \"0.1.1\"\n",
    "\n",
    "    pkg1 = f\"{group_id}:{artifact_id}:{version}\"\n",
    "\n",
    "    group_id = \"com.azure.cosmos.spark\"\n",
    "    artifact_id = \"azure-cosmos-spark_3-3_2-12\"\n",
    "    version = \"4.17.2\"\n",
    "\n",
    "    pkg2 = f\"{group_id}:{artifact_id}:{version}\"\n",
    "\n",
    "    return pkg1, pkg2\n",
    "\n",
    "\n",
    "######################    FILE MANAGEMENT       ###############################\n",
    "\n",
    "def list_files(startpath):\n",
    "\n",
    "    logging.info(\"listing files in directory\")    \n",
    "    list_files = []\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        for file in files:\n",
    "            #print(os.path.join(root, file))\n",
    "            list_files.append(os.path.join(root, file))\n",
    "    return [item for item in list_files if '.pdf' in item]\n",
    "\n",
    "def list_filepaths_in_cosmosdb_container():    \n",
    "       \n",
    "    logging.info(\"getting filepaths from cosmosdb\")    \n",
    "    resource_group_name = get_env_vars()['resource_group_name']\n",
    "    cosmosdb_acc = get_env_vars()['cosmosdb_acc']\n",
    "    database_name = get_env_vars()['database_name']\n",
    "    collection_name = get_env_vars()['collection_name'] \n",
    "    \n",
    "    try:\n",
    "        connecting_string = os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group {resource_group_name}\\\n",
    "                                --name {cosmosdb_acc} | jq .connectionStrings[0].connectionString \").read().strip().replace('\"','')\n",
    "        client = pymongo.MongoClient(connecting_string)\n",
    "        collection_client = client.get_database(database_name).get_collection(collection_name)\n",
    "        list = [item['Filepath'] for item in collection_client.find()]\n",
    "        return list\n",
    "    except Exception as e:\n",
    "        logging.error(f\"error getting filepaths from cosmosdb: {e}\")\n",
    "        raise Exception(f\"error getting filepaths from cosmosdb: {e}\")\n",
    "\n",
    "def list_pdfblobs():\n",
    "\n",
    "    logging.info(\"get environment variables\")\n",
    "    storage_account_name = get_env_vars()['storage_account_name']\n",
    "    resource_group_name = get_env_vars()['resource_group_name']\n",
    "    container_name = get_env_vars()['container_name']\n",
    "    \n",
    "    logging.info(\"listing pdf blobs in blob storage\")    \n",
    "    list_of_uploaded_files =  list_filepaths_in_cosmosdb_container()\n",
    "    storage_account_key = os.popen(f\"az storage account keys list -n {storage_account_name} -g {resource_group_name} --query [0].value -o tsv\").read().strip()\n",
    "    storage_connection_string = os.popen(f\"az storage account show-connection-string -g {resource_group_name} -n {storage_account_name} --query connectionString\").read().strip()  \n",
    "    container = ContainerClient.from_connection_string(conn_str=storage_connection_string, container_name=container_name)\n",
    "    blob_list = container.list_blobs()\n",
    "\n",
    "    blob_list =  [item['name'] for item in blob_list if item['name'].endswith('.pdf')  if item['name'] not in list_of_uploaded_files]\n",
    "    print(blob_list)\n",
    "    \n",
    "    return blob_list\n",
    "\n",
    "def delete_cosmosdb_uploaded_files():\n",
    "    \n",
    "    logging.info(\"getting cosmosdb keys from azure\")\n",
    "    resource_group_name = get_env_vars()['resource_group_name']\n",
    "    cosmosdb_acc = get_env_vars()['cosmosdb_acc']\n",
    "    database_name = get_env_vars()['database_name']\n",
    "    collection_name = get_env_vars()['collection_name']\n",
    "\n",
    "    connecting_string = os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group {resource_group_name}\\\n",
    "                              --name {cosmosdb_acc} | jq .connectionStrings[0].connectionString \").read().strip().replace('\"','')\n",
    "    collection_name = pymongo.MongoClient(connecting_string)[database_name][collection_name]\n",
    "    \n",
    "    return collection_name.delete_many({})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################    PDF EXTRACTION     ###############################\n",
    "\n",
    "def extract_title(pdf_path):\n",
    "    print(f\"extracting title from {pdf_path}\")\n",
    "    logging.info(\"extracting title from pdf\")\n",
    "    lst = pdf_path.replace('..','').split('/')[1:]\n",
    "    return lst\n",
    "\n",
    "# Extract text from a PDF file\n",
    "def preprocess_text(text):\n",
    "\n",
    "    if text is None or text == '':\n",
    "        logging.info(\"text is empty\")\n",
    "        return ''\n",
    "    else:\n",
    "        try:\n",
    "            logging.info(\"preprocessing text\")\n",
    "            # Replace any non-UTF-8 characters with a space\n",
    "            text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "            return text.strip()\n",
    "        except:\n",
    "            logging.error(\"error in preprocessing text\")\n",
    "            print(\"error in preprocessing text\")\n",
    "            return ''\n",
    "    \n",
    "\n",
    "def extract_text_from_container( list_of_pdf_uploaded =list_filepaths_in_cosmosdb_container()):\n",
    "    logging.info(\"extracting text from container\")    \n",
    "    storage_account_name = get_env_vars()['storage_account_name']\n",
    "    container_name = get_env_vars()['container_name']\n",
    "    resource_group_name = get_env_vars()['resource_group_name'] \n",
    "    storage_connection_string = os.popen(f\"az storage account show-connection-string -g {resource_group_name} -n {storage_account_name} --query connectionString\").read().strip()\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(storage_connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    try:\n",
    "        extracted_list = [(item.name,tika_parser(BlobServiceClient.from_connection_string(storage_connection_string).get_blob_client(container=container_name,blob= item.name).download_blob().content_as_bytes()))\\\n",
    "                        for item in \\\n",
    "                        ContainerClient.from_connection_string(conn_str=storage_connection_string, container_name=container_name).list_blobs()  \\\n",
    "                        if item.name not in list_of_pdf_uploaded and item.name.endswith('.pdf')] \n",
    "        return extracted_list\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"error extracting text from container: {e}\")\n",
    "        raise Exception(f\"error extracting text from container: {e}\")\n",
    "    \n",
    "# Extract text from a PDF file\n",
    "def load_blob_into_memory(blob_name):\n",
    "    \n",
    "    logging.info(\"loading blob into memory\")    \n",
    "    storage_account_name = get_env_vars()['storage_account_name']\n",
    "    container_name = get_env_vars()['container_name']\n",
    "    resource_group_name = get_env_vars()['resource_group_name']    \n",
    "    storage_connection_string = os.popen(f\"az storage account show-connection-string -g {resource_group_name} -n {storage_account_name} --query connectionString\").read().strip()\n",
    "\n",
    "    \n",
    "    if storage_account_name is None or container_name is None or resource_group_name is None:\n",
    "        logging.error(\"missing environment variables\")\n",
    "        raise Exception(\"Missing environment variables\")\n",
    "    elif blob_name is None:\n",
    "        logging.error(\"missing blob name\")\n",
    "        raise Exception(\"Missing blob name\")\n",
    "        return ''\n",
    "    else:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            logging.info(\"getting connection string\")\n",
    "            storage_connection_string = os.popen(f\"az storage account show-connection-string -g {resource_group_name} -n {storage_account_name} --query connectionString\").read().strip()\n",
    "\n",
    "            \n",
    "            logging.info(\"creating blob service client\")\n",
    "            blob_service_client = BlobServiceClient.from_connection_string(storage_connection_string)\n",
    "            \n",
    "            logging.info(\"creating blob client\")\n",
    "            blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)     \n",
    "            \n",
    "            # Check if blob exists\n",
    "            if blob_client.exists():\n",
    "                logging.info(f\"blob {blob_client.blob_name} exists\")                \n",
    "                blob_data = blob_client.download_blob().content_as_bytes()    \n",
    "                print(f\"Successfully downloaded blob: {blob_client.blob_name}\")\n",
    "                return blob_data\n",
    "            else:\n",
    "                logging.info(f\"blob {blob_client.blob_name} does not exist\")\n",
    "                logging.error(f\"blob {blob_client.blob_name} does not exist\")\n",
    "                print(f\"Blob {blob_client.blob_name} does not exist\")   \n",
    "               \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading blob: {e}\")\n",
    "\n",
    "# Extract text from a PDF file\n",
    "def tika_parser(blob_data):\n",
    "    logging.info(\"extracting text from pdf using tika\")\n",
    "    try:\n",
    "        with io.BytesIO(blob_data) as pdf_file:\n",
    "            \n",
    "            logging.info(\"trying to extract text using tika\")\n",
    "            try:\n",
    "                logging.info(\"Extracting text using tika\")\n",
    "                parsed_pdf = parser.from_buffer(pdf_file)\n",
    "                text = parsed_pdf['content']\n",
    "                print(f\"Successfully extracted {len(text)} characters from PDF using Tika\")\n",
    "                return text\n",
    "            except:\n",
    "                logging.error(\"error extracting text using tika\")\n",
    "                pass\n",
    "\n",
    "            # If Tika fails, try to extract text using pdfplumber\n",
    "            try:\n",
    "                logging.info(\"Extracting text using pdfplumber\")\n",
    "                with pdfplumber.load(pdf_file) as pdf:\n",
    "                    pages = pdf.pages\n",
    "                    text = \"\\n\".join([page.extract_text() for page in pages])\n",
    "                    print(f\"Successfully extracted {len(text)} characters from PDF using pdfplumber\")\n",
    "                    return text\n",
    "            except:\n",
    "                logging.error(\"error extracting text using pdfplumber\")\n",
    "                pass\n",
    "\n",
    "            # If both Tika and pdfplumber fail, return None\n",
    "            print(\"Failed to extract text from PDF using Tika or pdfplumber\")\n",
    "            return None\n",
    "    except:\n",
    "        logging.error(\"error reading pdf file from memory\")\n",
    "        print(\"Error reading PDF file from memory\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "\n",
    "def chatgpt3 (userinput, temperature=0.7, frequency_penalty=0, presence_penalty=0):\n",
    "    \"\"\" chat with gpt-3.5-turbo, the much cheaper version of gpt-3\"\"\"\n",
    "    \n",
    "    suffix = \"\\n\\nTl;dr\"\n",
    "    prompt = userinput+suffix\n",
    "    assistant_prompt =\"\"\n",
    "    message = [\n",
    "        {\"role\": \"user\", \"content\": prompt },        \n",
    "        {\"role\": \"system\", \"content\": \"you are a helpful distinguished scholarly assistant that uses efficient \\\n",
    "         communication to help finish the task of concisely summarizing an article by summarizing the most pertinent essence of the text as part of a paragraph. \\\n",
    "         use the fewest words as possible in english\"}\n",
    "         ]\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=temperature,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            messages=message\n",
    "        )\n",
    "        text = response['choices'][0]['message']['content']\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logging.error(\"error chatting with gpt-3.5-turbo for this error: {e}\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "                            \n",
    "    \n",
    "\n",
    "     \n",
    "def create_id(folder, typeofDoc, subject, author, title):\n",
    "    \n",
    "    logging.info(\"creating id field for cosmos db\")\n",
    "    # create a string to hash\n",
    "    my_string = f\"{folder}{typeofDoc}{subject}{author}{title}\"\n",
    "    # create a hash object using the SHA-256 algorithm\n",
    "    hash_object = hashlib.sha256()\n",
    "    # update the hash object with the string to be hashed\n",
    "    hash_object.update(my_string.encode())\n",
    "    # get the hexadecimal representation of the hash\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    return hex_dig\n",
    "\n",
    "######################    VECTOR DATABASE DATA LOADING     ###############################\n",
    "def split_text(text: str):\n",
    "    \"\"\" split text into chunks\"\"\"\n",
    "    logging.info(\"splitting text into chunks\")\n",
    "    text_splitter = CharacterTextSplitter()\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    " \n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    \"\"\" get embedding from openai\"\"\"\n",
    "    openai.api_key = get_env_vars()['OPENAI_API_KEY']    \n",
    "    return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "\n",
    "\n",
    "def get_pincone_pdfdata( text,metadata):    \n",
    "    \"\"\" get pinecone pdf data\"\"\"\n",
    "    #create list of vectors\n",
    "    chunks = split_text(text)   \n",
    "    \n",
    "    #create list of pinecone documents\n",
    "    pinecone_docs = [{\"id\": hashlib.sha256(item.encode()).hexdigest(),\n",
    "                        \"values\": [get_embedding(item )],\n",
    "                        \"metadata\": metadata\n",
    "                         } for item in chunks]  \n",
    "    return pinecone_docs\n",
    "\n",
    "def get_data_from_cosmosdb():\n",
    "    \"\"\" get data from cosmos db\"\"\"\n",
    "    #get cosmosdb connection string\n",
    "    resource_group_name = get_env_vars()['resource_group_name']\n",
    "    cosmosdb_acc = get_env_vars()['cosmosdb_acc']\n",
    "    database_name = get_env_vars()['database_name']\n",
    "    collection_name = get_env_vars()['collection_name']\n",
    "    connecting_string = os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group {resource_group_name}\\\n",
    "                          --name {cosmosdb_acc} | jq .connectionStrings[0].connectionString \").read().strip().replace('\"','')\n",
    "    \n",
    "    client = pymongo.MongoClient(connecting_string)\n",
    "    collection_client = client.get_database(database_name).get_collection(collection_name)\n",
    "\n",
    "    #create a list of tuples of text and metadata\n",
    "    try:\n",
    "        list = [(item['Metadata'], item['text']) for item in collection_client.find()]\n",
    "        print(list)\n",
    "        return list\n",
    "    except:\n",
    "        print('no data in cosmosdb')\n",
    "        return []\n",
    "   \n",
    "\n",
    "\n",
    "######################    UPLOAD DATA     ###############################\n",
    "\n",
    "def upsert_pinecone_data(vector):\n",
    "    \"\"\" upsert pinecone data\"\"\"\n",
    "    \"\"\" upsert pinecone index with pdf data\"\"\"\n",
    "    pinecone.apiKey = get_pinecone_keys()['pinecone.apiKey']\n",
    "    pinecone.environment = get_pinecone_keys()['pinecone.environment']\n",
    "    pinecone.indexName = get_pinecone_keys()['pinecone.indexName']\n",
    "    pinecone.projectName = get_pinecone_keys()['pinecone.projectName']\n",
    "\n",
    "    #initialize pinecone\n",
    "    pinecone.init(api_key=pinecone.apiKey, env=pinecone.environment)\n",
    "    index = pinecone.Index(pinecone.indexName) \n",
    "\n",
    "    #filter out ids with no metadata uploaded\n",
    "    # metadata_filter ={\"metadata\" : None}\n",
    "    try:\n",
    "        print('upsert vector')\n",
    "        return index.upsert(vector , namespace=pinecone.projectName)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e} - Skipping {vector['id']}\")\n",
    "        \n",
    "    # results = index.query(queries=[], metadata_filter=metadata_filter,top_k=None)\n",
    "    # ids = [result.id for result in results]\n",
    "\n",
    "    # if vector['id'] in ids:\n",
    "    #     try:\n",
    "    #         print('upsert vector')\n",
    "    #         return index.upsert(vector , namespace=pinecone.projectName)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Exception: {e} - Skipping {vector['id']}\")\n",
    "            \n",
    "    # else:\n",
    "    #     print('vector already exists')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def write_to_cosmosdb(items):\n",
    "    \n",
    "    logging.info(\"writing to cosmosdb\")\n",
    "    resource_group_name =  get_env_vars()['resource_group_name']\n",
    "    cosmosdb_acc =         get_env_vars()['cosmosdb_acc']\n",
    "    database_name =        get_env_vars()['database_name']\n",
    "    collection_name =      get_env_vars()['collection_name']\n",
    "    connecting_string = os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group {resource_group_name}\\\n",
    "                              --name {cosmosdb_acc} | jq '.connectionStrings[0].connectionString' \").read().strip().replace('\"','')\n",
    "    \n",
    "    logging.info(f\"connecting to cosmosdb {connecting_string}\")\n",
    "    mongo_client = pymongo.MongoClient(connecting_string)\n",
    "    collection = mongo_client[database_name][collection_name]\n",
    "\n",
    "    for item in items:\n",
    "        logging.info(f\"writing {item['id']} to cosmosdb\")\n",
    "        try:\n",
    "            print(item['summary'])\n",
    "            collection.update_one({\"id\": item[\"id\"]}, {\"$set\": item}, upsert=True)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception: {e} - Skipping {item['id']}\")\n",
    "            print(f\"Exception: {e} - Skipping {item['id']}\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set environment variables\n",
      "load blob storage\n",
      "[\"src/AcademicJournal/anthropology/Annaud/De l'intestin aux testicules Substances, humeurs et alliance tikar - Cameroun central.pdf\", 'src/AcademicJournal/anthropology/Jeffreys/Who are the Tikar.pdf', 'src/AcademicJournal/anthropology/Joseph/Tikar Stones.pdf', 'src/AcademicJournal/anthropology/Marguerat/Des montagnards entrepreneurs- les Bamileke du Cameroun.pdf', 'src/AcademicJournal/anthropology/Marguerat/Les peuples du Cameroun.pdf', 'src/AcademicJournal/anthropology/Price/Descent, Clans and Territorial Organization in the Tikar Chiefdom of Ngambe, Cameroon.pdf', 'src/AcademicJournal/anthropology/Price/WHO ARE THE TIKAR NOW.pdf', 'src/AcademicJournal/anthropology/Tchindakenfo/le probleme anglophone au cameroun - La reponse par le processus participatif au developpement territorial.pdf', 'src/AcademicJournal/anthropology/Warnier/Echanges, Developpement et hierarchies dans le bamenda pre-colonial - cameroun.pdf', 'src/AcademicJournal/anthropology/Warnier/The Grassfields of Cameroon Ancient Center or Periphery.pdf', 'src/AcademicJournal/anthropology/beemster/Les Tikar de Bankim.pdf', 'src/AcademicJournal/anthropology/chilver_kaberry/FROM TRIBUTE TO TAX IN A TIKAR CHIEFDOM.pdf', 'src/AcademicJournal/anthropology/hagege/Esquisse linguistique du Tikar, Cameroun (Claude Hagège).pdf', 'src/AcademicJournal/geopolitics/Nganang/The Amba Uprising Beyond Frances.pdf', 'src/AcademicJournal/history/Joseph/Dance Masks of the Tikar.pdf', 'src/AcademicJournal/history/Ndobegang_Mbapndah/COLONIAL BACKGROUND TO THE ECONOMIC EMPOWERMENT AND POLITICAL MOBILIZATION.pdf', 'src/OpED/philosophy/nganang/LE COMPLEXE DE SENGHOR.pdf', 'src/OpED/philosophy/nganang/Manifeste dune nouvelle littérature africaine Pour une écriture.pdf', 'src/OpED/political history/morin/La Révolution tranquille -réflexions personnelles.pdf', 'src/OpED/politics/Lamberton/Les Bamilékés dans le Cameroun d’aujourd’hui.pdf', 'src/excelSpreadsheet/Statistics/Romuald/EDS_ethnics Cameroon (3).pdf']\n",
      "number of pdf files: 1\n",
      "starting spark session\n",
      "23/03/28 00:59:05 WARN Utils: Your hostname, codespaces-670d97 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "23/03/28 00:59:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/codespace/.ivy2/cache\n",
      "The jars for the packages stored in: /home/codespace/.ivy2/jars\n",
      "io.pinecone#spark-pinecone_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5256e6fe-76b8-4cae-8c3b-59429b3a18db;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.pinecone#spark-pinecone_2.13;0.1.1 in central\n",
      "\tfound io.pinecone#pinecone-client;0.2.2 in central\n",
      "\tfound io.netty#netty-tcnative-boringssl-static;2.0.50.Final in central\n",
      "\tfound io.netty#netty-tcnative-classes;2.0.50.Final in central\n",
      "\tfound io.grpc#grpc-protobuf;1.47.0 in central\n",
      "\tfound io.grpc#grpc-api;1.47.0 in central\n",
      "\tfound io.grpc#grpc-context;1.47.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.10.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.19.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.0.1 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.47.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.47.0 in central\n",
      "\tfound com.google.guava#guava;31.0.1-android in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound org.checkerframework#checker-qual;3.12.0 in central\n",
      "\tfound org.checkerframework#checker-compat-qual;2.5.5 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound io.grpc#grpc-netty;1.47.0 in central\n",
      "\tfound io.grpc#grpc-core;1.47.0 in central\n",
      "\tfound io.netty#netty-codec-http2;4.1.72.Final in central\n",
      "\tfound io.netty#netty-common;4.1.72.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.72.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.72.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.72.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.72.Final in central\n",
      "\tfound io.netty#netty-handler;4.1.72.Final in central\n",
      "\tfound io.netty#netty-codec-http;4.1.72.Final in central\n",
      "\tfound com.google.code.gson#gson;2.9.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.19 in central\n",
      "\tfound io.perfmark#perfmark-api;0.25.0 in central\n",
      "\tfound io.netty#netty-handler-proxy;4.1.72.Final in central\n",
      "\tfound io.netty#netty-codec-socks;4.1.72.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.72.Final in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.8.3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.19.4 in central\n",
      ":: resolution report :: resolve 1054ms :: artifacts dl 37ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.8.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.9.0 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.10.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.0.1-android from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.19.4 from central in [default]\n",
      "\tio.grpc#grpc-api;1.47.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.47.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.47.0 from central in [default]\n",
      "\tio.grpc#grpc-netty;1.47.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.47.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.47.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.47.0 from central in [default]\n",
      "\tio.netty#netty-buffer;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-codec-http;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-codec-http2;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-codec-socks;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-handler-proxy;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-boringssl-static;2.0.50.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-classes;2.0.50.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.72.Final from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.25.0 from central in [default]\n",
      "\tio.pinecone#pinecone-client;0.2.2 from central in [default]\n",
      "\tio.pinecone#spark-pinecone_2.13;0.1.1 from central in [default]\n",
      "\torg.checkerframework#checker-compat-qual;2.5.5 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.12.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.19 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java;3.19.2 by [com.google.protobuf#protobuf-java;3.19.4] in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.0.1 by [com.google.api.grpc#proto-google-common-protos;2.8.3] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.13.0 by [com.google.protobuf#protobuf-java;3.19.2] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.7.1 by [com.google.errorprone#error_prone_annotations;2.10.0] in [default]\n",
      "\tio.netty#netty-tcnative-classes;2.0.46.Final by [io.netty#netty-tcnative-classes;2.0.50.Final] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   42  |   0   |   0   |   5   ||   37  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5256e6fe-76b8-4cae-8c3b-59429b3a18db\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 37 already retrieved (0kB/16ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 00:59:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"src/AcademicJournal/anthropology/Annaud/De l'intestin aux testicules Substances, humeurs et alliance tikar - Cameroun central.pdf\"]\n",
      "create the cosmosdb rdd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 00:59:18,582 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to /tmp/tika-server.jar.\n",
      "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to /tmp/tika-server.jar.\n",
      "2023-03-28 00:59:18,922 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to /tmp/tika-server.jar.md5.\n",
      "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to /tmp/tika-server.jar.md5.\n",
      "2023-03-28 00:59:19,084 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
      "WARNING:tika.tika:Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 50645 characters from PDF using Tika\n",
      "Successfully extracted 53836 characters from PDF using Tika\n",
      "Successfully extracted 43515 characters from PDF using Tika\n",
      "Successfully extracted 40730 characters from PDF using Tika\n",
      "Successfully extracted 75278 characters from PDF using Tika\n",
      "Successfully extracted 61900 characters from PDF using Tika\n",
      "Successfully extracted 32119 characters from PDF using Tika\n",
      "Successfully extracted 71598 characters from PDF using Tika\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:error extracting text using tika\n",
      "ERROR:root:error extracting text using pdfplumber\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 41671 characters from PDF using Tika\n",
      "Successfully extracted 25369 characters from PDF using Tika\n",
      "Successfully extracted 76719 characters from PDF using Tika\n",
      "Successfully extracted 127 characters from PDF using Tika\n",
      "Successfully extracted 47623 characters from PDF using Tika\n",
      "Successfully extracted 38913 characters from PDF using Tika\n",
      "Successfully extracted 39577 characters from PDF using Tika\n",
      "Successfully extracted 42013 characters from PDF using Tika\n",
      "Successfully extracted 535151 characters from PDF using Tika\n",
      "Successfully extracted 42015 characters from PDF using Tika\n",
      "Successfully extracted 33813 characters from PDF using Tika\n",
      "Successfully extracted 1150 characters from PDF using Tika\n",
      "number of pdf files: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"set environment variables\")\n",
    "os.environ['OPENAI_API_KEY'] = get_secret(\"chatKeys\", \"openaiKey\")   \n",
    "os.environ['storage_account_name'] = 'chatgptv2stn'\n",
    "os.environ['container_name'] = 'chatgpt-ctn'\n",
    "os.environ['resource_group_name'] ='chatgptGp'\n",
    "os.environ['cosmosdb_acc'] ='chatgptdb-acn'\n",
    "os.environ['database_name']='chatgptdb-dbn'\n",
    "os.environ['collection_name']='chatgptdb-cln'        \n",
    "pinecone_dict = get_pinecone_keys()\n",
    "pinecone_jar, cosmos_jar = set_spark_liraries()    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#blob storage loading\n",
    "print(\"load blob storage\")\n",
    "pdf_paths = [item for item in list_pdfblobs()][:1]\n",
    "print(f\"number of pdf files: {len(pdf_paths)}\")\n",
    "\n",
    "print(\"starting spark session\")    \n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"chatgpt\")\\\n",
    "    .config(\"spark.jars.packages\", f\"{pinecone_jar}\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(pdf_paths)        \n",
    "\n",
    "print(\"create the cosmosdb rdd\")\n",
    "preprocess_text_list =  spark.sparkContext.parallelize(extract_text_from_container()).collect()\n",
    "print(f\"number of pdf files: {len(preprocess_text_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocess_text_list[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def chatgpt3 (userinput, temperature=0.7, frequency_penalty=0, presence_penalty=0):\n",
    "    \"\"\" chat with gpt-3.5-turbo, the much cheaper version of gpt-3\"\"\"\n",
    "\n",
    "    if os.environ.get('OPENAI_API_KEY') is None:\n",
    "        logging.error(\"OpenAI API key not found. setting key by using get_env_vars()\")\n",
    "        openai.api_key = get_env_vars()['OPENAI_API_KEY']\n",
    "        \n",
    "    logging.info(\"chatting with gpt-3.5-turbo\")        \n",
    "    suffix = \"\\n\\nTl;dr\"\n",
    "    prompt = userinput+suffix\n",
    "    assistant_prompt =\"\"\n",
    "    message = [\n",
    "        {\"role\": \"user\", \"content\": prompt },        \n",
    "        {\"role\": \"system\", \"content\": \"you are a helpful distinguished scholarly assistant that uses efficient \\\n",
    "        communication to help finish the task of concisely summarizing an article by summarizing the most pertinent essence of the text as part of a paragraph. \\\n",
    "        use the fewest words as possible in english\"}\n",
    "        ]\n",
    "    try:\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=temperature,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            messages=message\n",
    "        )\n",
    "        text = response['choices'][0]['message']['content']\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"error chatting with gpt-3.5-turbo for this error: {e}\")\n",
    "        return ''\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cheaper_summarizer(text, title,temperature=0.7, frequency_penalty=0, presence_penalty=0):\n",
    "    \n",
    "    logging.info(\"summarizing text using gpt-3.5-turbo\")            \n",
    "    if text is None:\n",
    "        logging.info(f\"there is no text to summarize - Skipping {title}\")\n",
    "        print(f\"there is no text to summarize - Skipping {title}\")\n",
    "        return ''\n",
    "    else:\n",
    "        try:\n",
    "            logging.info(f\"summarizing {title} for {len(text)} characters\")\n",
    "            print(f\"Summarizing {title} for {len(text)} characters\")\n",
    "            #split text into chunks\n",
    "            chunks = split_text(text)\n",
    "            max_retry = 3\n",
    "            retry = 0\n",
    "            while retry < max_retry:\n",
    "                try:\n",
    "                    logging.info(f\"summarizing {title} for {len(text)} characters - attempt {retry}\")                    \n",
    "                    summaries = ' \\n'.join([chatgpt3(chunk, temperature=temperature, frequency_penalty=frequency_penalty, presence_penalty=presence_penalty) for chunk in chunks])\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Exception: {e} - Retrying {title}\")\n",
    "                    print(f\"Exception: {e} - Retrying {title}\") \n",
    "                    retry += 1\n",
    "                    sleep(5)\n",
    "                    continue                \n",
    "            return summaries\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception: {e} - Skipping {title}\")\n",
    "            print(f\"Exception: {e} - Skipping {title}\")\n",
    "            return ''\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']=get_env_vars()['OPENAI_API_KEY']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
