{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#                                                #   \n",
    "# this file will store all the utility functions #\n",
    "#                                                #\n",
    "#################################################\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import hashlib\n",
    "import datetime\n",
    "from time import time, sleep\n",
    "from typing import List\n",
    "\n",
    "import openai\n",
    "import pinecone\n",
    "import pdfplumber\n",
    "import tiktoken\n",
    "import pymongo\n",
    "from tika import parser\n",
    "from unidecode import unidecode\n",
    "from azure.cosmos import exceptions, CosmosClient, PartitionKey\n",
    "from azure.storage.blob import BlobServiceClient, ContainerClient, BlobClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.cosmos import CosmosClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import *\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "def cli_signIn():\n",
    "    \"\"\"function to login to azure cli if not already logged in\"\"\"\n",
    "    os.system(\"az account list --output tsv | grep True -q || az login\")\n",
    "\n",
    "######################    SECRETS       ###############################\n",
    "\n",
    "def get_env_vars():\n",
    "    \"\"\" get environment variables from azure\"\"\" \n",
    "  \n",
    "    env_dict = {\n",
    "        \"resource_group_name\": 'chatgptGp',\n",
    "        \"storage_account_name\": 'chatgptv2stn',\n",
    "        \"container_name\": 'chatgpt-ctn',\n",
    "        \"cosmosdb_acc\": 'chatgptdb-acn',\n",
    "        \"database_name\": 'chatgptdb-dbn',\n",
    "        \"collection_name\": 'chatgptdb-cln' ,\n",
    "        \"OPENAI_API_KEY\": get_secret(keyvault_name=\"chatkeys\",secret_name=\"openaiKey\"),\n",
    "        \"connection_string\": os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group 'chatgptGp'\\\n",
    "                              --name chatgptdb-acn | jq .connectionStrings[0].connectionString \").read().strip().replace('\"',''),\n",
    "    }   \n",
    "    for k, v in env_dict.items():\n",
    "        if v is None:\n",
    "            raise Exception(f\"{k} environment variable is not set\")\n",
    "    return env_dict\n",
    "\n",
    "\n",
    "def get_secret(keyvault_name =\"chatkeys\", secret_name = \"openaiKey\"):\n",
    "    \"\"\"Get secret from Azure Key Vault\"\"\"\n",
    "    credential = DefaultAzureCredential()\n",
    "    secret_client = SecretClient(vault_url=f\"https://{keyvault_name}.vault.azure.net\", credential=credential)\n",
    "    secret = secret_client.get_secret(secret_name)\n",
    "    return secret.value\n",
    "\n",
    "def get_pinecone_keys():\n",
    "    \"\"\" get pinecone keys from azure\"\"\"\n",
    "    pinecone_api_key = get_secret(keyvault_name =\"chatkeys\", secret_name = \"pinecone\")\n",
    "    pinecone_env = get_secret(keyvault_name =\"chatkeys\", secret_name = \"pineconeEnv\")\n",
    "    pinecone_index = get_secret(keyvault_name=\"chatKeys\", secret_name=\"pineconeIdx\")\n",
    "\n",
    "    return {\n",
    "        \"pinecone.apiKey\": pinecone_api_key,\n",
    "        \"pinecone.environment\": pinecone_env,\n",
    "        \"pinecone.indexName\": pinecone_index,\n",
    "        \"pinecone.projectName\": \"chatgpt3\"        \n",
    "    }\n",
    "        \n",
    "def get_cosmosdb_keys():\n",
    "    \"\"\" get cosmos db keys from azure\"\"\"\n",
    "    #create environment variables  \n",
    "    \n",
    "    resourceGroup = get_env_vars()['resource_group_name']\n",
    "    cosmosdb_name = get_env_vars()['cosmosdb_acc']\n",
    "    cosmosDbEndpoint_url = os.popen(f\"az cosmosdb show --resource-group {resourceGroup}  --name {cosmosdb_name} --query 'writeLocations[].documentEndpoint' -o tsv\").read().strip()\n",
    "    cosmos_account_key =   os.popen(f\"az cosmosdb keys  list --name {cosmosdb_name} --resource-group {resourceGroup} | jq -r '.primaryMasterKey'\").read().strip()    \n",
    "    database_name =        os.popen(f\"az cosmosdb database list --name {cosmosdb_name} --resource-group {resourceGroup} | jq -r '.[0].id'\").read().strip()\n",
    "    collection_name =       os.popen(f\"az cosmosdb collection list --name {cosmosdb_name} --db-name {database_name} --resource-group {resourceGroup} | jq -r '.[0].id'\").read().strip()\n",
    "    masterkey =            os.popen(f\" az cosmosdb list-keys --name {cosmosdb_name} --resource-group {resourceGroup} --query primaryMasterKey\").read().strip()\n",
    "    connection_string =    os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group {resourceGroup}\\\n",
    "                            --name {cosmosdb_name} | jq '.connectionStrings[0].connectionString' \").read().strip().replace('\"','')\n",
    "    \n",
    "    return {\n",
    "            \n",
    "            \"cosmosDbEndpoint_url\" : cosmosDbEndpoint_url,\n",
    "            \"masterkey\" : masterkey,\n",
    "            \"database_name\" : database_name,\n",
    "            \"collection_name\" : collection_name,\n",
    "            \"connection_string\" : connection_string\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def set_spark_liraries():\n",
    "        #packages to load in spark session\n",
    "        group_id = \"io.pinecone\"\n",
    "        artifact_id = \"spark-pinecone_2.13\"\n",
    "        version = \"0.1.1\"\n",
    "\n",
    "        pkg1 = f\"{group_id}:{artifact_id}:{version}\"\n",
    "\n",
    "        group_id = \"com.azure.cosmos.spark\"\n",
    "        artifact_id = \"azure-cosmos-spark_3-3_2-12\"\n",
    "        version = \"4.17.2\"\n",
    "\n",
    "        pkg2 = f\"{group_id}:{artifact_id}:{version}\"\n",
    "\n",
    "        return pkg1, pkg2\n",
    "\n",
    "\n",
    "######################    FILE MANAGEMENT       ###############################\n",
    "\n",
    "def list_files(startpath):\n",
    "    \"\"\" list all files in a directory\"\"\"\n",
    "    list_files = []\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        for file in files:\n",
    "            #print(os.path.join(root, file))\n",
    "            list_files.append(os.path.join(root, file))\n",
    "    return [item for item in list_files if '.pdf' in item]\n",
    "\n",
    "def list_filepaths_in_cosmosdb_container():\n",
    "    print(\"getting filepaths from cosmosdb\")   \n",
    "    \"\"\" get cosmos db keys from azure\"\"\"\n",
    "    resource_group_name = get_env_vars()['resource_group_name']\n",
    "    cosmosdb_acc = get_env_vars()['cosmosdb_acc']\n",
    "    database_name = get_env_vars()['database_name']\n",
    "    collection_name = get_env_vars()['collection_name'] \n",
    "    print(resource_group_name,cosmosdb_acc,database_name,collection_name)\n",
    "    \n",
    "    connecting_string = os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group {resource_group_name}\\\n",
    "                              --name {cosmosdb_acc} | jq .connectionStrings[0].connectionString \").read().strip().replace('\"','')\n",
    "    client = pymongo.MongoClient(connecting_string)\n",
    "    collection_client = client.get_database(database_name).get_collection(collection_name)\n",
    "    list = [item['Filepath'] for item in collection_client.find()]\n",
    "    return list\n",
    "\n",
    "def list_pdfblobs():\n",
    "\n",
    "    storage_account_name = get_env_vars()['storage_account_name']\n",
    "    resource_group_name = get_env_vars()['resource_group_name']\n",
    "    container_name = get_env_vars()['container_name']\n",
    "\n",
    "    \"\"\"list pdf blobs in blob storage\"\"\"\n",
    "    list_of_uploaded_files =  list_filepaths_in_cosmosdb_container()\n",
    "    storage_account_key = os.popen(f\"az storage account keys list -n {storage_account_name} -g {resource_group_name} --query [0].value -o tsv\").read().strip()\n",
    "    storage_connection_string = os.popen(f\"az storage account show-connection-string -g {resource_group_name} -n {storage_account_name} --query connectionString\").read().strip()  \n",
    "    container = ContainerClient.from_connection_string(conn_str=storage_connection_string, container_name=container_name)\n",
    "    blob_list = container.list_blobs()\n",
    "    #https://<your-storage-account-name>.blob.core.windows.net/<your-container-name>/<your-blob-name>\n",
    "\n",
    "    print(list_filepaths_in_cosmosdb_container()[:1])\n",
    "\n",
    "    blob_list =  [item['name'] for item in blob_list if item['name'].endswith('.pdf')  if item['name'] not in list_of_uploaded_files]\n",
    "    print(blob_list)\n",
    "    \n",
    "    return blob_list\n",
    "\n",
    "def delete_cosmosdb_uploaded_files():\n",
    "    \"\"\" get cosmos db keys from azure\"\"\"\n",
    "    resource_group_name = get_env_vars()['resource_group_name']\n",
    "    cosmosdb_acc = get_env_vars()['cosmosdb_acc']\n",
    "    database_name = get_env_vars()['database_name']\n",
    "    collection_name = get_env_vars()['collection_name']\n",
    "\n",
    "    connecting_string = os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group {resource_group_name}\\\n",
    "                              --name {cosmosdb_acc} | jq .connectionStrings[0].connectionString \").read().strip().replace('\"','')\n",
    "    collection_name = pymongo.MongoClient(connecting_string)[database_name][collection_name]\n",
    "    \n",
    "    return collection_name.delete_many({})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################    PDF EXTRACTION     ###############################\n",
    "\n",
    "def extract_title(pdf_path):\n",
    "    \"\"\" extract metatdata from a pdf path\"\"\"\n",
    "    lst = pdf_path.replace('..','').split('/')[1:]\n",
    "    return lst\n",
    "\n",
    "# Extract text from a PDF file\n",
    "def preprocess_text(text):\n",
    "    if text is None or text == '':\n",
    "        return ''\n",
    "    else:\n",
    "        try:\n",
    "            # Replace any non-UTF-8 characters with a space\n",
    "            text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "            return text.strip()\n",
    "        except:\n",
    "            print(\"error in preprocessing text\")\n",
    "            return ''\n",
    "    \n",
    "\n",
    "def extract_text_from_container( list_of_pdf_uploaded =list_filepaths_in_cosmosdb_container()):\n",
    "    print(\"get environment variables\")\n",
    "    storage_account_name = get_env_vars()['storage_account_name']\n",
    "    container_name = get_env_vars()['container_name']\n",
    "    resource_group_name = get_env_vars()['resource_group_name'] \n",
    "    storage_connection_string = os.popen(f\"az storage account show-connection-string -g {resource_group_name} -n {storage_account_name} --query connectionString\").read().strip()\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(storage_connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    return [(item.name,tika_parser(BlobServiceClient.from_connection_string(storage_connection_string).get_blob_client(container=container_name,blob= item.name).download_blob().content_as_bytes()))\\\n",
    "                for item in \\\n",
    "                ContainerClient.from_connection_string(conn_str=storage_connection_string, container_name=container_name).list_blobs()  \\\n",
    "                if item.name not in list_of_pdf_uploaded]\n",
    "                \n",
    "\n",
    "# Extract text from a PDF file\n",
    "def load_blob_into_memory(blob_name):\n",
    "    \"\"\"load blob into memory\"\"\"\n",
    "    \n",
    "    print(\"get environment variables\")\n",
    "    storage_account_name = get_env_vars()['storage_account_name']\n",
    "    container_name = get_env_vars()['container_name']\n",
    "    resource_group_name = get_env_vars()['resource_group_name']    \n",
    "    storage_connection_string = os.popen(f\"az storage account show-connection-string -g {resource_group_name} -n {storage_account_name} --query connectionString\").read().strip()\n",
    "\n",
    "    \n",
    "    if storage_account_name is None or container_name is None or resource_group_name is None:\n",
    "        raise Exception(\"Missing environment variables\")\n",
    "    elif blob_name is None:\n",
    "        raise Exception(\"Missing blob name\")\n",
    "        return ''\n",
    "    else:\n",
    "        \n",
    "        try:\n",
    "            #connection string\n",
    "            storage_connection_string = os.popen(f\"az storage account show-connection-string -g {resource_group_name} -n {storage_account_name} --query connectionString\").read().strip()\n",
    "\n",
    "            # Create blob service client\n",
    "            blob_service_client = BlobServiceClient.from_connection_string(storage_connection_string)\n",
    "            # Get blob client\n",
    "            blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)     \n",
    "            \n",
    "            # Check if blob exists\n",
    "            if blob_client.exists():\n",
    "                # Download blob data\n",
    "                blob_data = blob_client.download_blob().content_as_bytes()    \n",
    "                print(f\"Successfully downloaded blob: {blob_client.blob_name}\")\n",
    "                return blob_data\n",
    "            else:\n",
    "                print(f\"Blob {blob_client.blob_name} does not exist\")   \n",
    "               \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading blob: {e}\")\n",
    "\n",
    "# Extract text from a PDF file\n",
    "def tika_parser(blob_data):\n",
    "    try:\n",
    "        with io.BytesIO(blob_data) as pdf_file:\n",
    "            # Try to extract text using Tika parser\n",
    "            try:\n",
    "                parsed_pdf = parser.from_buffer(pdf_file)\n",
    "                text = parsed_pdf['content']\n",
    "                print(f\"Successfully extracted {len(text)} characters from PDF using Tika\")\n",
    "                return text\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # If Tika fails, try to extract text using pdfplumber\n",
    "            try:\n",
    "                with pdfplumber.load(pdf_file) as pdf:\n",
    "                    pages = pdf.pages\n",
    "                    text = \"\\n\".join([page.extract_text() for page in pages])\n",
    "                    print(f\"Successfully extracted {len(text)} characters from PDF using pdfplumber\")\n",
    "                    return text\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "\n",
    "            # If both Tika and pdfplumber fail, return None\n",
    "            print(\"Failed to extract text from PDF using Tika or pdfplumber\")\n",
    "            return None\n",
    "    except:\n",
    "        print(\"Error reading PDF file from memory\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "\n",
    "def chatgpt3 (userinput, temperature=0.7, frequency_penalty=0, presence_penalty=0):\n",
    "    \"\"\" chat with gpt-3.5-turbo, the much cheaper version of gpt-3\"\"\"\n",
    "    \n",
    "    suffix = \"\\n\\nTl;dr\"\n",
    "    prompt = userinput+suffix\n",
    "    assistant_prompt =\"\"\n",
    "    message = [\n",
    "        {\"role\": \"user\", \"content\": prompt },        \n",
    "        {\"role\": \"system\", \"content\": \"you are a helpful distinguished scholarly assistant that uses efficient \\\n",
    "         communication to help finish the task of concisely summarizing an article by summarizing the most pertinent essence of the text as part of a paragraph. \\\n",
    "         use the fewest words as possible in english\"}\n",
    "         ]\n",
    "    openai.api_key = get_env_vars()['OPENAI_API_KEY']\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=temperature,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        messages=message\n",
    "    )\n",
    "    text = response['choices'][0]['message']['content']\n",
    "    return text\n",
    "\n",
    "def cheaper_summarizer(text, title,temperature=0.7, frequency_penalty=0, presence_penalty=0,api_key=None):\n",
    "    \"\"\" chat with gpt-3.5-turbo, the much cheaper version of gpt-3\"\"\"        \n",
    "    \n",
    "    if text is None:\n",
    "        print(f\"there is no text to summarize - Skipping {title}\")\n",
    "        return ''\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"Summarizing {title} for {len(text)} characters\")\n",
    "            #split text into chunks\n",
    "            chunks = split_text(text)\n",
    "            max_retry = 3\n",
    "            retry = 0\n",
    "            while retry < max_retry:\n",
    "                try:\n",
    "                    summaries = ' \\n'.join([chatgpt3(chunk, temperature=temperature, frequency_penalty=frequency_penalty, presence_penalty=presence_penalty) for chunk in chunks])\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception: {e} - Retrying {title}\") \n",
    "                    retry += 1\n",
    "                    sleep(5)\n",
    "                    continue                \n",
    "            return summaries\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e} - Skipping {title}\")\n",
    "            return ''\n",
    "   \n",
    "                            \n",
    "\n",
    "\n",
    "     \n",
    "def create_id(folder, typeofDoc, subject, author, title):\n",
    "    \"\"\" create id field for cosmos db \"\"\"\n",
    "    # create a string to hash\n",
    "    my_string = f\"{folder}{typeofDoc}{subject}{author}{title}\"\n",
    "    # create a hash object using the SHA-256 algorithm\n",
    "    hash_object = hashlib.sha256()\n",
    "    # update the hash object with the string to be hashed\n",
    "    hash_object.update(my_string.encode())\n",
    "    # get the hexadecimal representation of the hash\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    return hex_dig\n",
    "\n",
    "######################    VECTOR DATABASE DATA LOADING     ###############################\n",
    "def split_text(text: str ,chunks: int = 1000):\n",
    "    \"\"\" split text into chunks\"\"\"\n",
    "    text_splitter = CharacterTextSplitter()\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    " \n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    \"\"\" get embedding from openai\"\"\"\n",
    "    openai.api_key = get_env_vars()['OPENAI_API_KEY']    \n",
    "    return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data_from_cosmosdb():\n",
    "    \"\"\" get data from cosmos db\"\"\"\n",
    "    #get cosmosdb connection string\n",
    "    resource_group_name = get_env_vars()['resource_group_name']\n",
    "    cosmosdb_acc = get_env_vars()['cosmosdb_acc']\n",
    "    database_name = get_env_vars()['database_name']\n",
    "    collection_name = get_env_vars()['collection_name']\n",
    "    connecting_string = os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group {resource_group_name}\\\n",
    "                          --name {cosmosdb_acc} | jq .connectionStrings[0].connectionString \").read().strip().replace('\"','')\n",
    "    \n",
    "    client = pymongo.MongoClient(connecting_string)\n",
    "    collection_client = client.get_database(database_name).get_collection(collection_name)\n",
    "\n",
    "    #create a list of tuples of text and metadata\n",
    "    try:\n",
    "        list = [[item['Metadata'],item['text']] for item in collection_client.find()]\n",
    "        print(list)\n",
    "        return list\n",
    "    except:\n",
    "        print('no data in cosmosdb')\n",
    "        return []\n",
    "   \n",
    "\n",
    "\n",
    "######################    UPLOAD DATA     ###############################\n",
    "\n",
    "def upsert_pinecone_data(vector):\n",
    "    \"\"\" upsert pinecone data\"\"\"\n",
    "    \"\"\" upsert pinecone index with pdf data\"\"\"\n",
    "    pinecone.apiKey = get_pinecone_keys()['pinecone.apiKey']\n",
    "    pinecone.environment = get_pinecone_keys()['pinecone.environment']\n",
    "    pinecone.indexName = get_pinecone_keys()['pinecone.indexName']\n",
    "    pinecone.projectName = get_pinecone_keys()['pinecone.projectName']\n",
    "\n",
    "    #initialize pinecone\n",
    "    pinecone.init(api_key=pinecone.apiKey, env=pinecone.environment)\n",
    "    index = pinecone.Index(pinecone.indexName) \n",
    "\n",
    "    #filter out ids with no metadata uploaded\n",
    "    # metadata_filter ={\"metadata\" : None}\n",
    "    try:\n",
    "        print('upsert vector')\n",
    "        return index.upsert(vector , namespace=pinecone.projectName)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e} - Skipping {vector['id']}\")\n",
    "        \n",
    "    # results = index.query(queries=[], metadata_filter=metadata_filter,top_k=None)\n",
    "    # ids = [result.id for result in results]\n",
    "\n",
    "    # if vector['id'] in ids:\n",
    "    #     try:\n",
    "    #         print('upsert vector')\n",
    "    #         return index.upsert(vector , namespace=pinecone.projectName)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Exception: {e} - Skipping {vector['id']}\")\n",
    "            \n",
    "    # else:\n",
    "    #     print('vector already exists')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def write_to_cosmosdb(items):\n",
    "    \n",
    "    resource_group_name =  get_env_vars()['resource_group_name']\n",
    "    cosmosdb_acc =         get_env_vars()['cosmosdb_acc']\n",
    "    database_name =        get_env_vars()['database_name']\n",
    "    collection_name =      get_env_vars()['collection_name']\n",
    "    connecting_string = os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group {resource_group_name}\\\n",
    "                              --name {cosmosdb_acc} | jq '.connectionStrings[0].connectionString' \").read().strip().replace('\"','')\n",
    "    \n",
    "    mongo_client = pymongo.MongoClient(connecting_string)\n",
    "    collection = mongo_client[database_name][collection_name]\n",
    "\n",
    "    for item in items:\n",
    "        print(item['summary'])\n",
    "        collection.update_one({\"id\": item[\"id\"]}, {\"$set\": item}, upsert=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"set environment variables\")\n",
    "os.environ['OPENAI_API_KEY'] = get_secret(\"chatKeys\", \"openaiKey\")   \n",
    "os.environ['storage_account_name'] = 'chatgptv2stn'\n",
    "os.environ['container_name'] = 'chatgpt-ctn'\n",
    "os.environ['resource_group_name'] ='chatgptGp'\n",
    "os.environ['cosmosdb_acc'] ='chatgptdb-acn'\n",
    "os.environ['database_name']='chatgptdb-dbn'\n",
    "os.environ['collection_name']='chatgptdb-cln'        \n",
    "pinecone_dict = get_pinecone_keys()\n",
    "pinecone_jar, cosmos_jar = set_spark_liraries()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"starting spark session\")    \n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"chatgpt\")\\\n",
    "    .config(\"spark.jars.packages\", f\"{pinecone_jar}\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_cosmosdb():\n",
    "    \"\"\" get data from cosmos db\"\"\"\n",
    "    #get cosmosdb connection string\n",
    "    resource_group_name = get_env_vars()['resource_group_name']\n",
    "    cosmosdb_acc = get_env_vars()['cosmosdb_acc']\n",
    "    database_name = get_env_vars()['database_name']\n",
    "    collection_name = get_env_vars()['collection_name']\n",
    "    connecting_string = os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group {resource_group_name}\\\n",
    "                          --name {cosmosdb_acc} | jq .connectionStrings[0].connectionString \").read().strip().replace('\"','')\n",
    "    \n",
    "    client = pymongo.MongoClient(connecting_string)\n",
    "    collection_client = client.get_database(database_name).get_collection(collection_name)\n",
    "\n",
    "    #create a list of tuples of text and metadata\n",
    "    try:\n",
    "        list = [item for item in collection_client.find()]\n",
    "        print(list)\n",
    "        return list\n",
    "    except:\n",
    "        print('no data in cosmosdb')\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group_name = get_env_vars()['resource_group_name']\n",
    "cosmosdb_acc = get_env_vars()['cosmosdb_acc']\n",
    "database_name = get_env_vars()['database_name']\n",
    "collection_name = get_env_vars()['collection_name']\n",
    "connecting_string = os.popen(f\"az cosmosdb keys list --type connection-strings --resource-group {resource_group_name}\\\n",
    "                      --name {cosmosdb_acc} | jq .connectionStrings[0].connectionString \").read().strip().replace('\"','')\n",
    "\n",
    "client = pymongo.MongoClient(connecting_string)\n",
    "collection_client = client.get_database(database_name).get_collection(collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pincone_pdfdata( text,metadata):    \n",
    "    \"\"\" get pinecone pdf data\"\"\"\n",
    "    #create list of vectors\n",
    "    chunks = split_text(text)   \n",
    "    \n",
    "    ids = [hashlib.sha256(item.encode()).hexdigest() for item in chunks]\n",
    "    embeds = [get_embedding(item) for item in chunks]\n",
    "    metadata = [metadata for item in chunks]\n",
    "    namespace = [metadata['title'] for item in chunks]\n",
    "    \n",
    "    #create list of pinecone documents\n",
    "    vectors = zip(ids,embeds,metadata,namespace)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "def create_hashid(text):\n",
    "    \"\"\" create id field for cosmos db \"\"\"\n",
    "    # create a string to hash\n",
    "    my_string = text\n",
    "    # create a hash object using the SHA-256 algorithm\n",
    "    hash_object = hashlib.sha256()\n",
    "    # update the hash object with the string to be hashed\n",
    "    hash_object.update(my_string.encode())\n",
    "    # get the hexadecimal representation of the hash\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    return hex_dig\n",
    "\n",
    "def create_pinecone_vectors(text,metadata):\n",
    "    \"\"\" create pinecone vectors \"\"\"\n",
    "    #create list of vectors\n",
    "    chunks = split_text(text)   \n",
    "    \n",
    "    #create list of pinecone documents\n",
    "    vectors = [( create_hashid(item),get_embedding(item ),metadata)\n",
    "                for item in chunks] \n",
    "    \n",
    "    return vectors\n",
    "\n",
    "def upsert_pinecone_data(vectors):\n",
    "    \"\"\" upsert pinecone data\"\"\"\n",
    "    \"\"\" upsert pinecone index with pdf data\"\"\"\n",
    "    pinecone.apiKey = get_pinecone_keys()['pinecone.apiKey']\n",
    "    pinecone.environment = get_pinecone_keys()['pinecone.environment']\n",
    "    pinecone.indexName = get_pinecone_keys()['pinecone.indexName']\n",
    "    pinecone.projectName = get_pinecone_keys()['pinecone.projectName']\n",
    "\n",
    "    #initialize pinecone\n",
    "    pinecone.init(api_key=pinecone.apiKey, env=pinecone.environment)\n",
    "    index = pinecone.Index(pinecone.indexName) \n",
    "    \n",
    "    #filter out ids with no metadata uploaded\n",
    "    # metadata_filter ={\"metadata\" : None}\n",
    "    try:\n",
    "        print('upsert vector')\n",
    "        with pinecone.Index(pinecone.indexName, pool_threads=30) as index:\n",
    "            #send request to pinecone\n",
    "            async_results = [\n",
    "                index.upsert(vectors=vector, async_req=True)\n",
    "                for vector in vectors\n",
    "            ]\n",
    "        # Wait for and retrieve responses (this raises in case of error)\n",
    "        [async_result.get() for async_result in async_results]\n",
    "        print('upsert complete')\n",
    "    except Exception as e:\n",
    "        print(f\"unable to upsert Exception: {e} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chatgpt-bams'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pinecone_keys()['pinecone.indexName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosomos_list  = [item for item in get_data_from_cosmosdb()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = cosomos_list[0]['Metadata']\n",
    "text = cosomos_list[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(text,chunks=100)\n",
    "print(f\"dimension of vector {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str='gpt-3.5-turbo') -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pinecone_vectors(text,metadata):\n",
    "    \"\"\" create pinecone vectors \"\"\"\n",
    "    #create list of vectors\n",
    "    chunks = split_text(text,chunks=1000)  \n",
    "\n",
    "    vector_dict = {}\n",
    "    vector_dict['vectors'] = [{'id' : create_hashid(item),                               \n",
    "                               'metadata':metadata,\n",
    "                               'values':get_embedding(item)}\n",
    "                                 for item in chunks]\n",
    "    vector_dict['namespace'] = metadata['title']\n",
    "    \n",
    "   \n",
    "    \n",
    "    return vector_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zip_pinecone_vectors(text,metadata):\n",
    "    \"\"\" create pinecone vectors \"\"\"\n",
    "    #create list of vectors\n",
    "    chunks = split_text(text,chunks=1000)  \n",
    "\n",
    "    ids = [create_hashid(item) for item in chunks]\n",
    "    embeds = [get_embedding(item) for item in chunks]\n",
    "    metadata = [metadata for item in chunks]\n",
    "    \n",
    "   \n",
    "    \n",
    "    return list(zip(ids,embeds,metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v =create_pinecone_vectors(text,metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = create_zip_pinecone_vectors(text,metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key= pinecone.apiKey, environment= pinecone.environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, FloatType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize(cosomos_list[:1]).\\\n",
    "    flatMap(lambda x: create_pinecone_vectors(x['text'],x['Metadata'])).\\\n",
    "        foreach( lambda x: upsert_pinecone_data(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
