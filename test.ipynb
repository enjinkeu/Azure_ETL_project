{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting filepaths from cosmosdb\n",
      "chatgptGp chatgptdb-acn chatgptdb-dbn chatgptdb-cln\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from utils import *\n",
    "sys.modules['pkg_resources'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set environment variables\n"
     ]
    }
   ],
   "source": [
    "print(\"set environment variables\")\n",
    "os.environ['OPENAI_API_KEY'] = get_secret(\"chatKeys\", \"openaiKey\")   \n",
    "os.environ['storage_account_name'] = 'chatgptv2stn'\n",
    "os.environ['container_name'] = 'chatgpt-ctn'\n",
    "os.environ['resource_group_name'] ='chatgptGp'\n",
    "os.environ['cosmosdb_acc'] ='chatgptdb-acn'\n",
    "os.environ['database_name']='chatgptdb-dbn'\n",
    "os.environ['collection_name']='chatgptdb-cln'        \n",
    "pinecone_dict = get_pinecone_keys()\n",
    "pinecone_jar, cosmos_jar = set_spark_liraries()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/26 19:29:39 WARN Utils: Your hostname, codespaces-670d97 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "23/03/26 19:29:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/codespace/.ivy2/cache\n",
      "The jars for the packages stored in: /home/codespace/.ivy2/jars\n",
      "io.pinecone#spark-pinecone_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f5aaf05f-46cc-453f-8af6-7914037bd443;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.pinecone#spark-pinecone_2.13;0.1.1 in central\n",
      "\tfound io.pinecone#pinecone-client;0.2.2 in central\n",
      "\tfound io.netty#netty-tcnative-boringssl-static;2.0.50.Final in central\n",
      "\tfound io.netty#netty-tcnative-classes;2.0.50.Final in central\n",
      "\tfound io.grpc#grpc-protobuf;1.47.0 in central\n",
      "\tfound io.grpc#grpc-api;1.47.0 in central\n",
      "\tfound io.grpc#grpc-context;1.47.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.10.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.19.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.0.1 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.47.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.47.0 in central\n",
      "\tfound com.google.guava#guava;31.0.1-android in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound org.checkerframework#checker-qual;3.12.0 in central\n",
      "\tfound org.checkerframework#checker-compat-qual;2.5.5 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound io.grpc#grpc-netty;1.47.0 in central\n",
      "\tfound io.grpc#grpc-core;1.47.0 in central\n",
      "\tfound io.netty#netty-codec-http2;4.1.72.Final in central\n",
      "\tfound io.netty#netty-common;4.1.72.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.72.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.72.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.72.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.72.Final in central\n",
      "\tfound io.netty#netty-handler;4.1.72.Final in central\n",
      "\tfound io.netty#netty-codec-http;4.1.72.Final in central\n",
      "\tfound com.google.code.gson#gson;2.9.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.19 in central\n",
      "\tfound io.perfmark#perfmark-api;0.25.0 in central\n",
      "\tfound io.netty#netty-handler-proxy;4.1.72.Final in central\n",
      "\tfound io.netty#netty-codec-socks;4.1.72.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.72.Final in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.8.3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.19.4 in central\n",
      ":: resolution report :: resolve 1348ms :: artifacts dl 39ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.8.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.9.0 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.10.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.0.1-android from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.19.4 from central in [default]\n",
      "\tio.grpc#grpc-api;1.47.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.47.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.47.0 from central in [default]\n",
      "\tio.grpc#grpc-netty;1.47.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.47.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.47.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.47.0 from central in [default]\n",
      "\tio.netty#netty-buffer;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-codec-http;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-codec-http2;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-codec-socks;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-handler-proxy;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-boringssl-static;2.0.50.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-classes;2.0.50.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.72.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.72.Final from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.25.0 from central in [default]\n",
      "\tio.pinecone#pinecone-client;0.2.2 from central in [default]\n",
      "\tio.pinecone#spark-pinecone_2.13;0.1.1 from central in [default]\n",
      "\torg.checkerframework#checker-compat-qual;2.5.5 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.12.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.19 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java;3.19.2 by [com.google.protobuf#protobuf-java;3.19.4] in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.0.1 by [com.google.api.grpc#proto-google-common-protos;2.8.3] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.13.0 by [com.google.protobuf#protobuf-java;3.19.2] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.7.1 by [com.google.errorprone#error_prone_annotations;2.10.0] in [default]\n",
      "\tio.netty#netty-tcnative-classes;2.0.46.Final by [io.netty#netty-tcnative-classes;2.0.50.Final] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   42  |   0   |   0   |   5   ||   37  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f5aaf05f-46cc-453f-8af6-7914037bd443\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 37 already retrieved (0kB/42ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/26 19:29:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName(\"chatgpt\")\\\n",
    "        .config(\"spark.jars.packages\", f\"{pinecone_jar}\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get environment variables\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 50645 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 53836 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 43515 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 40730 characters from PDF using Tika\n",
      "Successfully extracted 75278 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 61900 characters from PDF using Tika\n",
      "Successfully extracted 32119 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 71598 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 41671 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 25369 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 76719 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 127 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 47623 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 38913 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 39577 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 42013 characters from PDF using Tika\n",
      "Successfully extracted 535151 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 42015 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 33813 characters from PDF using Tika\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Failed to extract text from PDF using Tika or pdfplumber\n",
      "Successfully extracted 1150 characters from PDF using Tika\n"
     ]
    }
   ],
   "source": [
    "preprocess_text_list =  spark.sparkContext.parallelize(extract_text_from_container()).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "getting filepaths from cosmosdb                                     (0 + 4) / 4]\n",
      "getting filepaths from cosmosdb\n",
      "getting filepaths from cosmosdb\n",
      "getting filepaths from cosmosdb\n",
      "chatgptGp chatgptdb-acn chatgptdb-dbn chatgptdb-cln\n",
      "chatgptGp chatgptdb-acn chatgptdb-dbn chatgptdb-cln\n",
      "chatgptGp chatgptdb-acn chatgptdb-dbn chatgptdb-cln\n",
      "chatgptGp chatgptdb-acn chatgptdb-dbn chatgptdb-cln\n",
      "[Stage 5:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/26 19:44:12 ERROR Executor: Exception in task 3.0 in stage 5.0 (TID 23)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 1178, in func\n",
      "    r = f(it)\n",
      "  File \"/tmp/ipykernel_7652/3811326986.py\", line 20, in <lambda>\n",
      "  File \"/workspaces/codespaces-jupyter/utils.py\", line 465, in write_to_cosmosdb\n",
      "    for item in items:\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_7652/3811326986.py\", line 4, in <lambda>\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/03/26 19:44:12 ERROR TaskSetManager: Task 3 in stage 5.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (2 + 1) / 4]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 5.0 failed 1 times, most recent failure: Lost task 3.0 in stage 5.0 (TID 23) (92bf1a1f-7cfc-4d0e-8a98-914fd2ab575b.internal.cloudapp.net executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 1178, in func\n    r = f(it)\n  File \"/tmp/ipykernel_7652/3811326986.py\", line 20, in <lambda>\n  File \"/workspaces/codespaces-jupyter/utils.py\", line 465, in write_to_cosmosdb\n    for item in items:\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_7652/3811326986.py\", line 4, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 1178, in func\n    r = f(it)\n  File \"/tmp/ipykernel_7652/3811326986.py\", line 20, in <lambda>\n  File \"/workspaces/codespaces-jupyter/utils.py\", line 465, in write_to_cosmosdb\n    for item in items:\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_7652/3811326986.py\", line 4, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m\n\u001b[1;32m      1\u001b[0m spark\u001b[39m.\u001b[39;49msparkContext\u001b[39m.\u001b[39;49mparallelize(preprocess_text_list[\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m x: {\n\u001b[1;32m      2\u001b[0m                                             \u001b[39m\"\u001b[39;49m\u001b[39mFilepath\u001b[39;49m\u001b[39m\"\u001b[39;49m: x[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      3\u001b[0m                                             \u001b[39m\"\u001b[39;49m\u001b[39mMetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\n\u001b[1;32m      4\u001b[0m                                                 \u001b[39m\"\u001b[39;49m\u001b[39mfolder\u001b[39;49m\u001b[39m\"\u001b[39;49m: extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      5\u001b[0m                                                 \u001b[39m\"\u001b[39;49m\u001b[39mtypeofDoc\u001b[39;49m\u001b[39m\"\u001b[39;49m: extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m      6\u001b[0m                                                 \u001b[39m\"\u001b[39;49m\u001b[39msubject\u001b[39;49m\u001b[39m\"\u001b[39;49m: extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m2\u001b[39;49m],\n\u001b[1;32m      7\u001b[0m                                                 \u001b[39m\"\u001b[39;49m\u001b[39mauthor\u001b[39;49m\u001b[39m\"\u001b[39;49m: extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m3\u001b[39;49m],\n\u001b[1;32m      8\u001b[0m                                                 \u001b[39m\"\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m\"\u001b[39;49m: extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m4\u001b[39;49m]\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m      9\u001b[0m                                             },\n\u001b[1;32m     10\u001b[0m                                             \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m: x[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m     11\u001b[0m                                             \u001b[39m\"\u001b[39;49m\u001b[39msummary\u001b[39;49m\u001b[39m\"\u001b[39;49m: cheaper_summarizer(x[\u001b[39m1\u001b[39;49m], extract_title(x[\u001b[39m0\u001b[39;49m])),\n\u001b[1;32m     12\u001b[0m                                             \u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m: create_id(\n\u001b[1;32m     13\u001b[0m                                                 extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m0\u001b[39;49m],  \u001b[39m# folder\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m                                                 extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m1\u001b[39;49m],  \u001b[39m# typeofDoc\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m                                                 extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m2\u001b[39;49m],  \u001b[39m# subject\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m                                                 extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m3\u001b[39;49m],  \u001b[39m# author\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m                                                 extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m4\u001b[39;49m]\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m],  \u001b[39m# title\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m                                             ),\n\u001b[1;32m     19\u001b[0m                                             \u001b[39m\"\u001b[39;49m\u001b[39muploadDate\u001b[39;49m\u001b[39m\"\u001b[39;49m: datetime\u001b[39m.\u001b[39;49mdatetime\u001b[39m.\u001b[39;49mnow()\u001b[39m.\u001b[39;49mstrftime(\u001b[39m\"\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY-\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mm-\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mH:\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mM:\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mS\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m---> 20\u001b[0m                                         })\u001b[39m.\u001b[39;49mforeachPartition(\u001b[39mlambda\u001b[39;49;00m x: write_to_cosmosdb(x))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/rdd.py:1184\u001b[0m, in \u001b[0;36mRDD.foreachPartition\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1182\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m([])\n\u001b[0;32m-> 1184\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcount()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/rdd.py:1521\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m   1513\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[39m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1521\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m i: [\u001b[39msum\u001b[39;49m(\u001b[39m1\u001b[39;49m \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m i)])\u001b[39m.\u001b[39;49msum()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/rdd.py:1508\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[NumberOrArray]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNumberOrArray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1500\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m    6.0\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1508\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m x: [\u001b[39msum\u001b[39;49m(x)])\u001b[39m.\u001b[39;49mfold(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   1509\u001b[0m         \u001b[39m0\u001b[39;49m, operator\u001b[39m.\u001b[39;49madd\n\u001b[1;32m   1510\u001b[0m     )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/rdd.py:1336\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[39myield\u001b[39;00m acc\n\u001b[1;32m   1333\u001b[0m \u001b[39m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[39m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[39m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 1336\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m   1337\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1198\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 5.0 failed 1 times, most recent failure: Lost task 3.0 in stage 5.0 (TID 23) (92bf1a1f-7cfc-4d0e-8a98-914fd2ab575b.internal.cloudapp.net executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 1178, in func\n    r = f(it)\n  File \"/tmp/ipykernel_7652/3811326986.py\", line 20, in <lambda>\n  File \"/workspaces/codespaces-jupyter/utils.py\", line 465, in write_to_cosmosdb\n    for item in items:\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_7652/3811326986.py\", line 4, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 1178, in func\n    r = f(it)\n  File \"/tmp/ipykernel_7652/3811326986.py\", line 20, in <lambda>\n  File \"/workspaces/codespaces-jupyter/utils.py\", line 465, in write_to_cosmosdb\n    for item in items:\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_7652/3811326986.py\", line 4, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(preprocess_text_list[0]).map(lambda x: {\n",
    "                                            \"Filepath\": x[0],\n",
    "                                            \"Metadata\": {\n",
    "                                                \"folder\": extract_title(x[0])[0],\n",
    "                                                \"typeofDoc\": extract_title(x[0])[1],\n",
    "                                                \"subject\": extract_title(x[0])[2],\n",
    "                                                \"author\": extract_title(x[0])[3],\n",
    "                                                \"title\": extract_title(x[0])[4].split('.')[0]\n",
    "                                            },\n",
    "                                            \"text\": x[1],\n",
    "                                            \"summary\": cheaper_summarizer(x[1], extract_title(x[0])),\n",
    "                                            \"id\": create_id(\n",
    "                                                extract_title(x[0])[0],  # folder\n",
    "                                                extract_title(x[0])[1],  # typeofDoc\n",
    "                                                extract_title(x[0])[2],  # subject\n",
    "                                                extract_title(x[0])[3],  # author\n",
    "                                                extract_title(x[0])[4].split('.')[0],  # title\n",
    "                                            ),\n",
    "                                            \"uploadDate\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                                        }).foreachPartition(lambda x: write_to_cosmosdb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "getting filepaths from cosmosdb                                     (0 + 4) / 4]\n",
      "getting filepaths from cosmosdb\n",
      "getting filepaths from cosmosdb\n",
      "getting filepaths from cosmosdb\n",
      "chatgptGp chatgptdb-acn chatgptdb-dbn chatgptdb-cln\n",
      "chatgptGp chatgptdb-acn chatgptdb-dbn chatgptdb-cln\n",
      "chatgptGp chatgptdb-acn chatgptdb-dbn chatgptdb-cln\n",
      "chatgptGp chatgptdb-acn chatgptdb-dbn chatgptdb-cln\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/26 19:40:22 ERROR Executor: Exception in task 1.0 in stage 3.0 (TID 13)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 1178, in func\n",
      "    r = f(it)\n",
      "  File \"/tmp/ipykernel_7652/986849879.py\", line 21, in <lambda>\n",
      "  File \"/workspaces/codespaces-jupyter/utils.py\", line 465, in write_to_cosmosdb\n",
      "    for item in items:\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_7652/986849879.py\", line 5, in <lambda>\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/03/26 19:40:22 ERROR TaskSetManager: Task 1 in stage 3.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==============>                                            (1 + 1) / 4]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 13) (92bf1a1f-7cfc-4d0e-8a98-914fd2ab575b.internal.cloudapp.net executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 1178, in func\n    r = f(it)\n  File \"/tmp/ipykernel_7652/986849879.py\", line 21, in <lambda>\n  File \"/workspaces/codespaces-jupyter/utils.py\", line 465, in write_to_cosmosdb\n    for item in items:\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_7652/986849879.py\", line 5, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 1178, in func\n    r = f(it)\n  File \"/tmp/ipykernel_7652/986849879.py\", line 21, in <lambda>\n  File \"/workspaces/codespaces-jupyter/utils.py\", line 465, in write_to_cosmosdb\n    for item in items:\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_7652/986849879.py\", line 5, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m load_to_cosmosdb_rdd \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49msparkContext\u001b[39m.\u001b[39;49mparallelize(preprocess_text_list[\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m x: (x[\u001b[39m0\u001b[39;49m], preprocess_text(x[\u001b[39m1\u001b[39;49m])))\u001b[39m.\u001b[39;49m\\\n\u001b[1;32m      2\u001b[0m                                     \u001b[39mmap\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m x: {\n\u001b[1;32m      3\u001b[0m                                             \u001b[39m\"\u001b[39;49m\u001b[39mFilepath\u001b[39;49m\u001b[39m\"\u001b[39;49m: x[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      4\u001b[0m                                             \u001b[39m\"\u001b[39;49m\u001b[39mMetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\n\u001b[1;32m      5\u001b[0m                                                 \u001b[39m\"\u001b[39;49m\u001b[39mfolder\u001b[39;49m\u001b[39m\"\u001b[39;49m: extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      6\u001b[0m                                                 \u001b[39m\"\u001b[39;49m\u001b[39mtypeofDoc\u001b[39;49m\u001b[39m\"\u001b[39;49m: extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m      7\u001b[0m                                                 \u001b[39m\"\u001b[39;49m\u001b[39msubject\u001b[39;49m\u001b[39m\"\u001b[39;49m: extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m2\u001b[39;49m],\n\u001b[1;32m      8\u001b[0m                                                 \u001b[39m\"\u001b[39;49m\u001b[39mauthor\u001b[39;49m\u001b[39m\"\u001b[39;49m: extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m3\u001b[39;49m],\n\u001b[1;32m      9\u001b[0m                                                 \u001b[39m\"\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m\"\u001b[39;49m: extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m4\u001b[39;49m]\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m     10\u001b[0m                                             },\n\u001b[1;32m     11\u001b[0m                                             \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m: x[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m     12\u001b[0m                                             \u001b[39m\"\u001b[39;49m\u001b[39msummary\u001b[39;49m\u001b[39m\"\u001b[39;49m: cheaper_summarizer(x[\u001b[39m1\u001b[39;49m], extract_title(x[\u001b[39m0\u001b[39;49m])),\n\u001b[1;32m     13\u001b[0m                                             \u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m: create_id(\n\u001b[1;32m     14\u001b[0m                                                 extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m0\u001b[39;49m],  \u001b[39m# folder\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m                                                 extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m1\u001b[39;49m],  \u001b[39m# typeofDoc\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m                                                 extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m2\u001b[39;49m],  \u001b[39m# subject\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m                                                 extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m3\u001b[39;49m],  \u001b[39m# author\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m                                                 extract_title(x[\u001b[39m0\u001b[39;49m])[\u001b[39m4\u001b[39;49m]\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m],  \u001b[39m# title\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m                                             ),\n\u001b[1;32m     20\u001b[0m                                             \u001b[39m\"\u001b[39;49m\u001b[39muploadDate\u001b[39;49m\u001b[39m\"\u001b[39;49m: datetime\u001b[39m.\u001b[39;49mdatetime\u001b[39m.\u001b[39;49mnow()\u001b[39m.\u001b[39;49mstrftime(\u001b[39m\"\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY-\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mm-\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mH:\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mM:\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mS\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m---> 21\u001b[0m                                         })\u001b[39m.\u001b[39;49mforeachPartition(\u001b[39mlambda\u001b[39;49;00m x: write_to_cosmosdb(x))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/rdd.py:1184\u001b[0m, in \u001b[0;36mRDD.foreachPartition\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1182\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m([])\n\u001b[0;32m-> 1184\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcount()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/rdd.py:1521\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m   1513\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[39m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1521\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m i: [\u001b[39msum\u001b[39;49m(\u001b[39m1\u001b[39;49m \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m i)])\u001b[39m.\u001b[39;49msum()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/rdd.py:1508\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[NumberOrArray]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNumberOrArray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1500\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m    6.0\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1508\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m x: [\u001b[39msum\u001b[39;49m(x)])\u001b[39m.\u001b[39;49mfold(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   1509\u001b[0m         \u001b[39m0\u001b[39;49m, operator\u001b[39m.\u001b[39;49madd\n\u001b[1;32m   1510\u001b[0m     )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/rdd.py:1336\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[39myield\u001b[39;00m acc\n\u001b[1;32m   1333\u001b[0m \u001b[39m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[39m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[39m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 1336\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m   1337\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1198\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 13) (92bf1a1f-7cfc-4d0e-8a98-914fd2ab575b.internal.cloudapp.net executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 1178, in func\n    r = f(it)\n  File \"/tmp/ipykernel_7652/986849879.py\", line 21, in <lambda>\n  File \"/workspaces/codespaces-jupyter/utils.py\", line 465, in write_to_cosmosdb\n    for item in items:\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_7652/986849879.py\", line 5, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pyspark/rdd.py\", line 1178, in func\n    r = f(it)\n  File \"/tmp/ipykernel_7652/986849879.py\", line 21, in <lambda>\n  File \"/workspaces/codespaces-jupyter/utils.py\", line 465, in write_to_cosmosdb\n    for item in items:\n  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_7652/986849879.py\", line 5, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "load_to_cosmosdb_rdd = spark.sparkContext.parallelize(preprocess_text_list[0]).map(lambda x: (x[0], preprocess_text(x[1]))).\\\n",
    "                                    map(lambda x: {\n",
    "                                            \"Filepath\": x[0],\n",
    "                                            \"Metadata\": {\n",
    "                                                \"folder\": extract_title(x[0])[0],\n",
    "                                                \"typeofDoc\": extract_title(x[0])[1],\n",
    "                                                \"subject\": extract_title(x[0])[2],\n",
    "                                                \"author\": extract_title(x[0])[3],\n",
    "                                                \"title\": extract_title(x[0])[4].split('.')[0]\n",
    "                                            },\n",
    "                                            \"text\": x[1],\n",
    "                                            \"summary\": cheaper_summarizer(x[1], extract_title(x[0])),\n",
    "                                            \"id\": create_id(\n",
    "                                                extract_title(x[0])[0],  # folder\n",
    "                                                extract_title(x[0])[1],  # typeofDoc\n",
    "                                                extract_title(x[0])[2],  # subject\n",
    "                                                extract_title(x[0])[3],  # author\n",
    "                                                extract_title(x[0])[4].split('.')[0],  # title\n",
    "                                            ),\n",
    "                                            \"uploadDate\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                                        }).foreachPartition(lambda x: write_to_cosmosdb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    #set variables to be used in this ETL process\n",
    "\n",
    "    print(\"set environment variables\")\n",
    "    os.environ['OPENAI_API_KEY'] = get_secret(\"chatKeys\", \"openaiKey\")   \n",
    "    os.environ['storage_account_name'] = 'chatgptv2stn'\n",
    "    os.environ['container_name'] = 'chatgpt-ctn'\n",
    "    os.environ['resource_group_name'] ='chatgptGp'\n",
    "    os.environ['cosmosdb_acc'] ='chatgptdb-acn'\n",
    "    os.environ['database_name']='chatgptdb-dbn'\n",
    "    os.environ['collection_name']='chatgptdb-cln'        \n",
    "    pinecone_dict = get_pinecone_keys()\n",
    "    pinecone_jar, cosmos_jar = set_spark_liraries()    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    #blob storage loading\n",
    "\n",
    "    print(\"load blob storage\")\n",
    "    pdf_paths = [item for item in list_pdfblobs()][:1]\n",
    "    print(f\"number of pdf files: {len(pdf_paths)}\")\n",
    "\n",
    "    \n",
    "    print(\"starting spark session\")    \n",
    "    spark = SparkSession.builder\\\n",
    "        .appName(\"chatgpt\")\\\n",
    "        .config(\"spark.jars.packages\", f\"{pinecone_jar}\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    print(pdf_paths)        \n",
    "    \n",
    "    print(\"create the cosmosdb rdd\")\n",
    "\n",
    "    preprocess_text_list =  spark.sparkContext.parallelize(extract_text_from_container()).collect()\n",
    "\n",
    "    \n",
    "    load_to_cosmosdb_rdd = spark.sparkContext.parallelize(preprocess_text_list).map(lambda x: (x[0], preprocess_text(x[1]))).\\\n",
    "                                    map(lambda x: {\n",
    "                                            \"Filepath\": x[0],\n",
    "                                            \"Metadata\": {\n",
    "                                                \"folder\": extract_title(x[0])[0],\n",
    "                                                \"typeofDoc\": extract_title(x[0])[1],\n",
    "                                                \"subject\": extract_title(x[0])[2],\n",
    "                                                \"author\": extract_title(x[0])[3],\n",
    "                                                \"title\": extract_title(x[0])[4].split('.')[0]\n",
    "                                            },\n",
    "                                            \"text\": x[1],\n",
    "                                            \"summary\": cheaper_summarizer(x[1], extract_title(x[0])),\n",
    "                                            \"id\": create_id(\n",
    "                                                extract_title(x[0])[0],  # folder\n",
    "                                                extract_title(x[0])[1],  # typeofDoc\n",
    "                                                extract_title(x[0])[2],  # subject\n",
    "                                                extract_title(x[0])[3],  # author\n",
    "                                                extract_title(x[0])[4].split('.')[0],  # title\n",
    "                                            ),\n",
    "                                            \"uploadDate\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                                        }).foreachPartition(lambda x: write_to_cosmosdb(x))\n",
    "    \n",
    "\n",
    "    # print(\"create the pinecone rdd and write to pinecone\")\n",
    "    # load_data_to_pinecone_rdff = spark.sparkContext.parallelize(get_data_from_cosmosdb()).map(lambda x : ([x['text'],x['Metadata']])).\\\n",
    "    #                                             map(lambda x : get_pincone_pdfdata(x[0],x[1])).\\\n",
    "    #                                             foreach(lambda x : upsert_pinecone_data(x))\n",
    "                                                \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
